{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720e4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math #important library , calculate log\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd950e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientists have discovered a new species of marine life in the deep ocean.',\n",
       " \"NASA's Mars rover is searching for signs of ancient life on the Red Planet.\",\n",
       " 'The stock market experienced a significant drop in trading today.',\n",
       " 'Astronomers have identified a distant galaxy with unusual star formations.',\n",
       " 'The government announced new measures to combat climate change.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample collection of documents\n",
    "documents = [\n",
    "    \"Scientists have discovered a new species of marine life in the deep ocean.\",\n",
    "    \"NASA's Mars rover is searching for signs of ancient life on the Red Planet.\",\n",
    "    \"The stock market experienced a significant drop in trading today.\",\n",
    "    \"Astronomers have identified a distant galaxy with unusual star formations.\",\n",
    "    \"The government announced new measures to combat climate change.\"\n",
    "]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ea50c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species': 'species',\n",
       " 'oceans': 'ocean',\n",
       " \"ocean's\": 'ocean',\n",
       " 'rover': 'rover',\n",
       " 'discovered': 'discover',\n",
       " 'experienced': 'experience',\n",
       " 'rovers': 'rover',\n",
       " 'trading': 'trade',\n",
       " 'identified': 'identify',\n",
       " 'identifies': 'identify',\n",
       " 'formations': 'formation',\n",
       " 'governments': 'government',\n",
       " 'measures': 'measure'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary for lemmatization (a simple example, not comprehensive)\n",
    "lemmatization_dict = {\n",
    "    \"species\": \"specie\",\n",
    "    \"species\": \"species\",\n",
    "    \"oceans\": \"ocean\",\n",
    "    \"ocean's\": \"ocean\",\n",
    "    \"rover\": \"rover\",\n",
    "    \"discovered\":\"discover\",\n",
    "    \"experienced\":\"experience\",\n",
    "    \"rovers\": \"rover\",\n",
    "    \"trading\": \"trade\",\n",
    "    \"identified\": \"identify\",\n",
    "    \"identifies\": \"identify\",\n",
    "    \"formations\": \"formation\",\n",
    "    \"governments\": \"government\",\n",
    "    \"measures\": \"measure\"\n",
    "}\n",
    "\n",
    "lemmatization_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f1f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed247b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents into words (terms), remove punctuation, and lemmatize\n",
    "def preprocess_text(document):\n",
    "    terms = document.lower().split()\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34cee75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ancient',\n",
       " 'announced',\n",
       " 'astronomers',\n",
       " 'change',\n",
       " 'climate',\n",
       " 'combat',\n",
       " 'deep',\n",
       " 'discover',\n",
       " 'distant',\n",
       " 'drop',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'formation',\n",
       " 'galaxy',\n",
       " 'government',\n",
       " 'have',\n",
       " 'identify',\n",
       " 'in',\n",
       " 'is',\n",
       " 'life',\n",
       " 'marine',\n",
       " 'market',\n",
       " 'mars',\n",
       " 'measure',\n",
       " \"nasa's\",\n",
       " 'new',\n",
       " 'ocean',\n",
       " 'of',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'red',\n",
       " 'rover',\n",
       " 'scientists',\n",
       " 'searching',\n",
       " 'significant',\n",
       " 'signs',\n",
       " 'species',\n",
       " 'star',\n",
       " 'stock',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'trade',\n",
       " 'unusual',\n",
       " 'with'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of unique terms (vocabulary)\n",
    "vocabulary = set()\n",
    "# you code here ...\n",
    "for document in documents:\n",
    "    uterms = preprocess_text(document)\n",
    "    vocabulary.update(uterms)\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0c442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': [0, 0, 0, 0, 0],\n",
       " 'formation': [0, 0, 0, 0, 0],\n",
       " 'on': [0, 0, 0, 0, 0],\n",
       " 'trade': [0, 0, 0, 0, 0],\n",
       " 'experience': [0, 0, 0, 0, 0],\n",
       " 'rover': [0, 0, 0, 0, 0],\n",
       " 'have': [0, 0, 0, 0, 0],\n",
       " 'announced': [0, 0, 0, 0, 0],\n",
       " 'significant': [0, 0, 0, 0, 0],\n",
       " 'combat': [0, 0, 0, 0, 0],\n",
       " 'for': [0, 0, 0, 0, 0],\n",
       " 'stock': [0, 0, 0, 0, 0],\n",
       " 'species': [0, 0, 0, 0, 0],\n",
       " 'signs': [0, 0, 0, 0, 0],\n",
       " 'climate': [0, 0, 0, 0, 0],\n",
       " 'of': [0, 0, 0, 0, 0],\n",
       " 'identify': [0, 0, 0, 0, 0],\n",
       " 'distant': [0, 0, 0, 0, 0],\n",
       " 'ocean': [0, 0, 0, 0, 0],\n",
       " 'ancient': [0, 0, 0, 0, 0],\n",
       " 'marine': [0, 0, 0, 0, 0],\n",
       " 'life': [0, 0, 0, 0, 0],\n",
       " 'with': [0, 0, 0, 0, 0],\n",
       " 'in': [0, 0, 0, 0, 0],\n",
       " 'astronomers': [0, 0, 0, 0, 0],\n",
       " 'change': [0, 0, 0, 0, 0],\n",
       " 'discover': [0, 0, 0, 0, 0],\n",
       " 'new': [0, 0, 0, 0, 0],\n",
       " 'drop': [0, 0, 0, 0, 0],\n",
       " 'deep': [0, 0, 0, 0, 0],\n",
       " \"nasa's\": [0, 0, 0, 0, 0],\n",
       " 'a': [0, 0, 0, 0, 0],\n",
       " 'planet': [0, 0, 0, 0, 0],\n",
       " 'government': [0, 0, 0, 0, 0],\n",
       " 'unusual': [0, 0, 0, 0, 0],\n",
       " 'measure': [0, 0, 0, 0, 0],\n",
       " 'galaxy': [0, 0, 0, 0, 0],\n",
       " 'today': [0, 0, 0, 0, 0],\n",
       " 'is': [0, 0, 0, 0, 0],\n",
       " 'searching': [0, 0, 0, 0, 0],\n",
       " 'star': [0, 0, 0, 0, 0],\n",
       " 'red': [0, 0, 0, 0, 0],\n",
       " 'scientists': [0, 0, 0, 0, 0],\n",
       " 'market': [0, 0, 0, 0, 0],\n",
       " 'the': [0, 0, 0, 0, 0],\n",
       " 'mars': [0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store the term frequency (TF) for each term in each document\n",
    "tf_values = {term: [0] * len(documents) for term in vocabulary}\n",
    "\n",
    "tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc32da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'formation': [0.1, 0, 0, 0, 0],\n",
       " 'on': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'trade': [0.1, 0, 0, 0, 0],\n",
       " 'experience': [0.1, 0, 0, 0, 0],\n",
       " 'rover': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'have': [0.1, 0, 0, 0, 0],\n",
       " 'announced': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'significant': [0.1, 0, 0, 0, 0],\n",
       " 'combat': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'for': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'stock': [0.1, 0, 0, 0, 0],\n",
       " 'species': [0.07692307692307693, 0, 0, 0, 0],\n",
       " 'signs': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'climate': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'of': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'identify': [0.1, 0, 0, 0, 0],\n",
       " 'distant': [0.1, 0, 0, 0, 0],\n",
       " 'ocean': [0.07692307692307693, 0, 0, 0, 0],\n",
       " 'ancient': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'marine': [0.07692307692307693, 0, 0, 0, 0],\n",
       " 'life': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'with': [0.1, 0, 0, 0, 0],\n",
       " 'in': [0.1, 0, 0, 0, 0],\n",
       " 'astronomers': [0.1, 0, 0, 0, 0],\n",
       " 'change': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'discover': [0.07692307692307693, 0, 0, 0, 0],\n",
       " 'new': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'drop': [0.1, 0, 0, 0, 0],\n",
       " 'deep': [0.07692307692307693, 0, 0, 0, 0],\n",
       " \"nasa's\": [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'a': [0.1, 0, 0, 0, 0],\n",
       " 'planet': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'government': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'unusual': [0.1, 0, 0, 0, 0],\n",
       " 'measure': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'galaxy': [0.1, 0, 0, 0, 0],\n",
       " 'today': [0.1, 0, 0, 0, 0],\n",
       " 'is': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'searching': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'star': [0.1, 0, 0, 0, 0],\n",
       " 'red': [0.07142857142857142, 0, 0, 0, 0],\n",
       " 'scientists': [0.07692307692307693, 0, 0, 0, 0],\n",
       " 'market': [0.1, 0, 0, 0, 0],\n",
       " 'the': [0.1111111111111111, 0, 0, 0, 0],\n",
       " 'mars': [0.07142857142857142, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Term Frequency (TF)\n",
    "for i, document in enumerate(documents):\n",
    "    # your code here ....\n",
    "    pre_doc = preprocess_text(document)\n",
    "    for term in pre_doc:\n",
    "        if term in pre_doc:\n",
    "            tf_values[term][0] = +1\n",
    "        tf_values[term][0] = tf_values[term][0] / len(pre_doc)\n",
    "\n",
    "tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d91fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1.6094379124341003,\n",
       " 'formation': 1.6094379124341003,\n",
       " 'on': 1.6094379124341003,\n",
       " 'trade': 1.6094379124341003,\n",
       " 'experience': 1.6094379124341003,\n",
       " 'rover': 1.6094379124341003,\n",
       " 'have': 0.9162907318741551,\n",
       " 'announced': 1.6094379124341003,\n",
       " 'significant': 1.6094379124341003,\n",
       " 'combat': 1.6094379124341003,\n",
       " 'for': 1.6094379124341003,\n",
       " 'stock': 1.6094379124341003,\n",
       " 'species': 1.6094379124341003,\n",
       " 'signs': 1.6094379124341003,\n",
       " 'climate': 1.6094379124341003,\n",
       " 'of': 0.9162907318741551,\n",
       " 'identify': 1.6094379124341003,\n",
       " 'distant': 1.6094379124341003,\n",
       " 'ocean': 1.6094379124341003,\n",
       " 'ancient': 1.6094379124341003,\n",
       " 'marine': 1.6094379124341003,\n",
       " 'life': 0.9162907318741551,\n",
       " 'with': 1.6094379124341003,\n",
       " 'in': 0.9162907318741551,\n",
       " 'astronomers': 1.6094379124341003,\n",
       " 'change': 1.6094379124341003,\n",
       " 'discover': 1.6094379124341003,\n",
       " 'new': 0.9162907318741551,\n",
       " 'drop': 1.6094379124341003,\n",
       " 'deep': 1.6094379124341003,\n",
       " \"nasa's\": 1.6094379124341003,\n",
       " 'a': 0.5108256237659907,\n",
       " 'planet': 1.6094379124341003,\n",
       " 'government': 1.6094379124341003,\n",
       " 'unusual': 1.6094379124341003,\n",
       " 'measure': 1.6094379124341003,\n",
       " 'galaxy': 1.6094379124341003,\n",
       " 'today': 1.6094379124341003,\n",
       " 'is': 1.6094379124341003,\n",
       " 'searching': 1.6094379124341003,\n",
       " 'star': 1.6094379124341003,\n",
       " 'red': 1.6094379124341003,\n",
       " 'scientists': 1.6094379124341003,\n",
       " 'market': 1.6094379124341003,\n",
       " 'the': 0.22314355131420976,\n",
       " 'mars': 1.6094379124341003}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf_values = {}\n",
    "total_documents = len(documents)\n",
    "for term in vocabulary:\n",
    "    for uterms in vocabulary:\n",
    "        document_occurred = sum([term in preprocess_text(document) for document in documents])\n",
    "    idf_values[term] = math.log(total_documents / document_occurred)\n",
    "\n",
    "idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be824663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.17882643471490003,\n",
       "  0.16094379124341004,\n",
       "  0.11495985088815001,\n",
       "  0.16094379124341004,\n",
       "  0.16094379124341004,\n",
       "  0.11495985088815001,\n",
       "  0.09162907318741552,\n",
       "  0.17882643471490003,\n",
       "  0.16094379124341004,\n",
       "  0.17882643471490003,\n",
       "  0.11495985088815001,\n",
       "  0.16094379124341004,\n",
       "  0.12380291634108465,\n",
       "  0.11495985088815001,\n",
       "  0.17882643471490003,\n",
       "  0.06544933799101108,\n",
       "  0.16094379124341004,\n",
       "  0.16094379124341004,\n",
       "  0.12380291634108465,\n",
       "  0.11495985088815001,\n",
       "  0.12380291634108465,\n",
       "  0.06544933799101108,\n",
       "  0.16094379124341004,\n",
       "  0.09162907318741552,\n",
       "  0.16094379124341004,\n",
       "  0.17882643471490003,\n",
       "  0.12380291634108465,\n",
       "  0.10181008131935056,\n",
       "  0.16094379124341004,\n",
       "  0.12380291634108465,\n",
       "  0.11495985088815001,\n",
       "  0.051082562376599076,\n",
       "  0.11495985088815001,\n",
       "  0.17882643471490003,\n",
       "  0.16094379124341004,\n",
       "  0.17882643471490003,\n",
       "  0.16094379124341004,\n",
       "  0.16094379124341004,\n",
       "  0.11495985088815001,\n",
       "  0.11495985088815001,\n",
       "  0.16094379124341004,\n",
       "  0.11495985088815001,\n",
       "  0.12380291634108465,\n",
       "  0.16094379124341004,\n",
       "  0.024793727923801082,\n",
       "  0.11495985088815001],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF-IDF values\n",
    "tfidf_values = []\n",
    "for i, document in enumerate(documents):\n",
    "    terms = preprocess_text(document)\n",
    "    tfidf_document = []\n",
    "    for term in vocabulary:\n",
    "        tf = tf_values[term][i]\n",
    "        idf = idf_values[term]\n",
    "        tfidf = tf * idf\n",
    "        tfidf_document.append(tfidf)\n",
    "    tfidf_values.append(tfidf_document)\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f53d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "         to  formation       on     trade  experience    rover      have  \\\n",
      "0  0.178826   0.160944  0.11496  0.160944    0.160944  0.11496  0.091629   \n",
      "1  0.000000   0.000000  0.00000  0.000000    0.000000  0.00000  0.000000   \n",
      "2  0.000000   0.000000  0.00000  0.000000    0.000000  0.00000  0.000000   \n",
      "3  0.000000   0.000000  0.00000  0.000000    0.000000  0.00000  0.000000   \n",
      "4  0.000000   0.000000  0.00000  0.000000    0.000000  0.00000  0.000000   \n",
      "\n",
      "   announced  significant    combat  ...    galaxy     today       is  \\\n",
      "0   0.178826     0.160944  0.178826  ...  0.160944  0.160944  0.11496   \n",
      "1   0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.00000   \n",
      "2   0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.00000   \n",
      "3   0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.00000   \n",
      "4   0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.00000   \n",
      "\n",
      "   searching      star      red  scientists    market       the     mars  \n",
      "0    0.11496  0.160944  0.11496    0.123803  0.160944  0.024794  0.11496  \n",
      "1    0.00000  0.000000  0.00000    0.000000  0.000000  0.000000  0.00000  \n",
      "2    0.00000  0.000000  0.00000    0.000000  0.000000  0.000000  0.00000  \n",
      "3    0.00000  0.000000  0.00000    0.000000  0.000000  0.000000  0.00000  \n",
      "4    0.00000  0.000000  0.00000    0.000000  0.000000  0.000000  0.00000  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF values to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_values, columns=list(vocabulary))\n",
    "\n",
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fcbe8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF results to a CSV file (optional)\n",
    "# df_tfidf.to_csv(\"tfidf_custom_preprocessed_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36077c4e",
   "metadata": {},
   "source": [
    "# Using Libraries for Lemmatization and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0495e332",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\IR codes\\lab3-tf-idf-deborahsom\\Lab 3 - tf-idf.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# if your machine doesn't have these libraries, you need to install them\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# if your machine doesn't have these libraries, you need to install them\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69087c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\IR codes\\lab3-tf-idf-deborahsom\\Lab 3 - tf-idf.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize NLTK's lemmatizer and download stopwords\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Initialize NLTK's lemmatizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize NLTK's lemmatizer and download stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# Initialize NLTK's lemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize documents into words (terms), remove punctuation, lemmatize, and remove stopwords\n",
    "def preprocess_text(document):\n",
    "    terms = nltk.word_tokenize(document)\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [ps.stem(term) for term in terms]\n",
    "    terms = [term.lower() for term in terms if term not in stopwords.words('english')]\n",
    "    return ' '.join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c050db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\IR codes\\lab3-tf-idf-deborahsom\\Lab 3 - tf-idf.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     preprocessed_documents\u001b[39m.\u001b[39mappend(preprocess_text(document))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Create a TfidfVectorizer instance\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/IR%20codes/lab3-tf-idf-deborahsom/Lab%203%20-%20tf-idf.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in the documents\n",
    "preprocessed_documents = []\n",
    "for document in documents:\n",
    "    preprocessed_documents.append(preprocess_text(document))\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ded965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed documents to compute TF-IDF values CADT@0zJanZ!\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e4c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "    ancient   announc  astronom     chang    climat    combat     deep  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.37007   \n",
      "1  0.339992  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
      "3  0.000000  0.000000  0.377964  0.000000  0.000000  0.000000  0.00000   \n",
      "4  0.000000  0.387757  0.000000  0.387757  0.387757  0.387757  0.00000   \n",
      "\n",
      "    discov   distant      drop  ...  scientist    search      sign  signific  \\\n",
      "0  0.37007  0.000000  0.000000  ...    0.37007  0.000000  0.000000  0.000000   \n",
      "1  0.00000  0.000000  0.000000  ...    0.00000  0.339992  0.339992  0.000000   \n",
      "2  0.00000  0.000000  0.377964  ...    0.00000  0.000000  0.000000  0.377964   \n",
      "3  0.00000  0.377964  0.000000  ...    0.00000  0.000000  0.000000  0.000000   \n",
      "4  0.00000  0.000000  0.000000  ...    0.00000  0.000000  0.000000  0.000000   \n",
      "\n",
      "     speci      star     stock     today     trade     unusu  \n",
      "0  0.37007  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.00000  0.000000  0.377964  0.377964  0.377964  0.000000  \n",
      "3  0.00000  0.377964  0.000000  0.000000  0.000000  0.377964  \n",
      "4  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
